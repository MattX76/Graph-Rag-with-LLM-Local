# üìö Local RAG Chatbot con Knowledge Graphs

Asistente conversacional que funciona completamente de forma local, combinando **modelos de lenguaje** y **b√∫squeda inteligente** para ofrecer respuestas precisas basadas en tus documentos.  
Integra b√∫squeda h√≠brida (base estructural + FAISS (base vectorial para busqueda semantica)), construcci√≥n de grafos de conocimiento y un sistema de memoria para mantener el contexto.  
Soporta la carga de archivos PDF, DOCX y TXT, procesando la informaci√≥n sin depender de servidores externos.

---

## üîß Requisitos Previos

Antes de comenzar, aseg√∫rate de contar con:
- **Python 3.10+** instalado en tu sistema.
- **pip** actualizado.
- **Ollama** instalado para la gesti√≥n de modelos locales.
- (Opcional) **Docker** si prefieres una implementaci√≥n en contenedores.

---

## üõ† Instalaci√≥n Paso a Paso

### 1Ô∏è‚É£ Instalaci√≥n Manual con Python y Entorno Virtual
```bash
git clone 
cd Local-RAG-KG-Chatbot

# Crear entorno virtual
python -m venv venv

# Activar entorno
# En Windows:
venv\Scripts\activate
# En macOS/Linux:
source venv/bin/activate

# (Opcional) Actualizar pip
pip install --upgrade pip

# Instalar dependencias
pip install -r requirements.txt
2Ô∏è‚É£ Configuraci√≥n de Ollama
Descarga e instala Ollama ‚Üí https://ollama.com/

Descarga los modelos necesarios:

```bash
Copiar
Editar
ollama pull deepseek-r1:7b
ollama pull nomic-embed-text
Si quieres utilizar otros modelos, modifica las variables MODEL o EMBEDDINGS_MODEL en el archivo .env.

### 3Ô∏è‚É£ Ejecuci√≥n de la Aplicaci√≥n
```bash
ollama serve
streamlit run app.py
```
Abre tu navegador en http://localhost:xxxx para interactuar con el asistente.

üê≥ Instalaci√≥n con Docker
Opci√≥n A: Usar Ollama desde tu m√°quina (host)
```bash
Copiar
Editar
docker-compose build
docker-compose up
La aplicaci√≥n se abrir√° en http://localhost:8501.

Opci√≥n B: Todo en contenedores (Ollama + Chatbot)
yaml
Copiar
Editar
version: "3.8"

services:
  ollama:
    image: ghcr.io/jmorganca/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"

  rag-chatbot:
    container_name: rag-chatbot
    build: .
    ports:
      - "8501:8501"
    environment:
      - OLLAMA_API_URL=http://ollama:11434
      - MODEL=deepseek-r1:7b
      - EMBEDDINGS_MODEL=nomic-embed-text:latest
      - CROSS_ENCODER_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2
    depends_on:
      - ollama
Luego ejecuta:

docker-compose build
docker-compose up
üß† C√≥mo Funciona
Sube tus documentos (PDF, DOCX, TXT).

Recuperaci√≥n h√≠brida: BM25 y FAISS localizan los fragmentos m√°s relevantes.

GraphRAG: crea un grafo de conocimiento para entender relaciones y contexto.

Reordenamiento neuronal con Cross-Encoder para priorizar resultados.

Expansi√≥n de consultas (HyDE) para mejorar la precisi√≥n.

Memoria conversacional para mantener el hilo del di√°logo.

Generaci√≥n final con el modelo seleccionado en Ollama.
