# ğŸ“š Local RAG Chatbot con Knowledge Graphs

Asistente conversacional que funciona completamente de forma local, combinando **modelos de lenguaje** y **bÃºsqueda inteligente** para ofrecer respuestas precisas basadas en tus documentos.  
Integra bÃºsqueda hÃ­brida (base estructural + FAISS (base vectorial para busqueda semantica)), construcciÃ³n de grafos de conocimiento y un sistema de memoria para mantener el contexto.  
Soporta la carga de archivos PDF, DOCX y TXT, procesando la informaciÃ³n sin depender de servidores externos.

---

## ğŸ”§ Requisitos Previos

Antes de empezar, asegÃºrate de tener:  

- ğŸ **Python 3.10+**  
- ğŸ“¦ **pip** actualizado  
- ğŸ¤– **Ollama** para la gestiÃ³n de modelos locales  
- ğŸ³ (Opcional) **Docker** para contenedores  

---

## ğŸ›  InstalaciÃ³n Paso a Paso

### 1ï¸âƒ£ InstalaciÃ³n Manual con Python y Entorno Virtual

```bash
git clone https://github.com/MattX76/Graph-Rag-with-LLM-Local

# Crear entorno virtual
python -m venv venv

# Activar entorno
# Windows:
venv\Scripts\activate
# macOS/Linux:
source venv/bin/activate

# Actualizar pip (opcional)
pip install --upgrade pip

# Instalar dependencias
pip install -r requirements.txt
```
### 2ï¸âƒ£ ConfiguraciÃ³n de Ollama
Descargar e instalar Ollama â†’ https://ollama.com/

Descargar modelos necesarios:
```bash
ollama pull deepseek-r1:7b
ollama pull nomic-embed-text
```
ğŸ”§ Nota: Para otros modelos, modifica MODEL o EMBEDDINGS_MODEL en .env.

### 3ï¸âƒ£ EjecuciÃ³n de la AplicaciÃ³n
```bash
ollama serve
streamlit run app.py
```
ğŸŒ Abre tu navegador en http://localhost:xxxx

### ğŸ³ InstalaciÃ³n con Docker
OpciÃ³n A: Usar Ollama desde tu mÃ¡quina (host)
```bash
docker-compose build
docker-compose up
OpciÃ³n B: Todo en contenedores (Ollama + Chatbot)
```
```yaml
version: "3.8"

services:
  ollama:
    image: ghcr.io/jmorganca/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"

  rag-chatbot:
    container_name: rag-chatbot
    build: .
    ports:
      - "8501:8501"
    environment:
      - OLLAMA_API_URL=http://ollama:11434
      - MODEL=deepseek-r1:7b
      - EMBEDDINGS_MODEL=nomic-embed-text:latest
      - CROSS_ENCODER_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2
    depends_on:
      - ollama
```
```bash
docker-compose build
docker-compose up
```
### ğŸŒ La aplicaciÃ³n estarÃ¡ disponible en http://localhost:8501

ğŸ§  CÃ³mo Funciona
ğŸ“„ Subida de documentos (PDF, DOCX, TXT)

ğŸ” RecuperaciÃ³n hÃ­brida: BM25 + FAISS localizan fragmentos relevantes

ğŸ§© GraphRAG: crea un grafo de conocimiento para entender contexto y relaciones

ğŸ§  Reordenamiento neuronal con Cross-Encoder

ğŸ’¡ ExpansiÃ³n de consultas (HyDE) para mayor precisiÃ³n

ğŸ—‚ Memoria conversacional para mantener el hilo del diÃ¡logo

ğŸ¤– GeneraciÃ³n final con el modelo seleccionado en Ollama
